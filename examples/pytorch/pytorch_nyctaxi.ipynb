{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Fare Prediction with RayDP and Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import os\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import raydp\n",
    "from raydp.torch.estimator import TorchEstimator\n",
    "from raydp.utils import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize or connect to existed Ray cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 22:11:18,018\tINFO services.py:1169 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.5',\n",
       " 'raylet_ip_address': '192.168.1.5',\n",
       " 'redis_address': '192.168.1.5:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-12-16_22-11-17_484866_84597/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-12-16_22-11-17_484866_84597/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-12-16_22-11-17_484866_84597',\n",
       " 'metrics_export_port': 65506,\n",
       " 'node_id': '240c6e958529c67e9f52ec04a73e37d0c3726d61'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Firstly, You need to init or connect to a ray cluster. Note that you should set include_java to True.\n",
    "# For more config info in ray, please refer the ray doc. https://docs.ray.io/en/latest/package-ref.html\n",
    "# ray.init(address=\"auto\", redis_password=\"123\")\n",
    "#ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m SLF4J: Class path contains multiple SLF4J bindings.\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m SLF4J: Found binding in [jar:file:/Users/dr6jl/Documents/ray-raydp/ray/python/ray/jars/ray_dist.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m SLF4J: Found binding in [jar:file:/Users/dr6jl/anaconda3/lib/python3.8/site-packages/pyspark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m WARNING: An illegal reflective access operation has occurred\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/dr6jl/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "# After initialize ray cluster, you can use the raydp api to get a spark session\n",
    "app_name = \"NYC Taxi Fare Prediction with RayDP\"\n",
    "num_executors = 1\n",
    "cores_per_executor = 1\n",
    "memory_per_executor = \"2GB\"\n",
    "spark = raydp.init_spark(app_name, num_executors, cores_per_executor, memory_per_executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed data preprocessing with pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then you can code as you are using spark\n",
    "# The dataset can be downloaded from https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data\n",
    "# Here we just use a subset of the training data\n",
    "train = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"../../data1/new-york-city-taxi-fare-prediction/train.csv\")\n",
    "\n",
    "# Set spark timezone for processing datetime\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the outlier\n",
    "def clean_up(data):\n",
    "    \n",
    "    data = data.filter(col('pickup_longitude')<=-72) \\\n",
    "            .filter(col('pickup_longitude')>=-76) \\\n",
    "            .filter(col('dropoff_longitude')<=-72) \\\n",
    "            .filter(col('dropoff_longitude')>=-76) \\\n",
    "            .filter(col('pickup_latitude')<=42) \\\n",
    "            .filter(col('pickup_latitude')>=38) \\\n",
    "            .filter(col('dropoff_latitude')<=42) \\\n",
    "            .filter(col('dropoff_latitude')>=38) \\\n",
    "            .filter(col('passenger_count')<=6) \\\n",
    "            .filter(col('passenger_count')>=1) \\\n",
    "            .filter(col('fare_amount') > 0) \\\n",
    "            .filter(col('fare_amount') < 250) \\\n",
    "            .filter(col('dropoff_longitude') != col('pickup_longitude')) \\\n",
    "            .filter(col('dropoff_latitude') != col('pickup_latitude')) \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time related features\n",
    "def add_time_features(data):\n",
    "    data = data.withColumn(\"day\", dayofmonth(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"hour_of_day\", hour(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"day_of_week\", dayofweek(col(\"pickup_datetime\"))-2)\n",
    "    data = data.withColumn(\"week_of_year\", weekofyear(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"month_of_year\", month(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"quarter_of_year\", quarter(col(\"pickup_datetime\")))\n",
    "    data = data.withColumn(\"year\", year(col(\"pickup_datetime\")))\n",
    "    @udf(\"int\")\n",
    "    def night(hour, weekday):\n",
    "        if ((hour <= 20) and (hour >= 16) and (weekday < 5)):\n",
    "            return int(1)\n",
    "        else:\n",
    "            return int(0)\n",
    "    @udf(\"int\")\n",
    "    def late_night(hour):\n",
    "        if ((hour <= 6) and (hour >= 20)):\n",
    "            return int(1)\n",
    "        else:\n",
    "            return int(0)\n",
    "    data = data.withColumn(\"night\", night(\"hour_of_day\", \"day_of_week\"))\n",
    "    data = data.withColumn(\"late_night\", late_night(\"hour_of_day\"))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add distance related features\n",
    "def add_distance_features(data):\n",
    "    @udf(\"float\")\n",
    "    def manhattan(lat1, lon1, lat2, lon2):\n",
    "        return float(np.abs(lat2 - lat1) + np.abs(lon2 - lon1))\n",
    "    # Location of NYC downtown\n",
    "    ny = (-74.0063889, 40.7141667)\n",
    "    # Location of the three airport in NYC\n",
    "    jfk = (-73.7822222222, 40.6441666667)\n",
    "    ewr = (-74.175, 40.69)\n",
    "    lgr = (-73.87, 40.77)\n",
    "    # Features about the distance between pickup/dropoff and airport\n",
    "    data = data.withColumn(\"abs_diff_longitude\", abs(col(\"dropoff_longitude\")-col(\"pickup_longitude\"))) \\\n",
    "            .withColumn(\"abs_diff_latitude\", abs(col(\"dropoff_latitude\") - col(\"pickup_latitude\")))\n",
    "    data = data.withColumn(\"manhattan\", col(\"abs_diff_latitude\")+col(\"abs_diff_longitude\"))\n",
    "    data = data.withColumn(\"pickup_distance_jfk\", manhattan(\"pickup_longitude\", \"pickup_latitude\", lit(jfk[0]), lit(jfk[1])))\n",
    "    data = data.withColumn(\"dropoff_distance_jfk\", manhattan(\"dropoff_longitude\", \"dropoff_latitude\", lit(jfk[0]), lit(jfk[1])))\n",
    "    data = data.withColumn(\"pickup_distance_ewr\", manhattan(\"pickup_longitude\", \"pickup_latitude\", lit(ewr[0]), lit(ewr[1])))\n",
    "    data = data.withColumn(\"dropoff_distance_ewr\", manhattan(\"dropoff_longitude\", \"dropoff_latitude\", lit(ewr[0]), lit(ewr[1])))\n",
    "    data = data.withColumn(\"pickup_distance_lgr\", manhattan(\"pickup_longitude\", \"pickup_latitude\", lit(lgr[0]), lit(lgr[1])))\n",
    "    data = data.withColumn(\"dropoff_distance_lgr\", manhattan(\"dropoff_longitude\", \"dropoff_latitude\", lit(lgr[0]), lit(lgr[1])))\n",
    "    data = data.withColumn(\"pickup_distance_downtown\", manhattan(\"pickup_longitude\", \"pickup_latitude\", lit(ny[0]), lit(ny[1])))\n",
    "    data = data.withColumn(\"dropoff_distance_downtown\", manhattan(\"dropoff_longitude\", \"dropoff_latitude\", lit(ny[0]), lit(ny[1])))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused features\n",
    "def drop_col(data):\n",
    "    \n",
    "    data = data.drop(\"pickup_datetime\") \\\n",
    "            .drop(\"pickup_longitude\") \\\n",
    "            .drop(\"pickup_latitude\") \\\n",
    "            .drop(\"dropoff_longitude\") \\\n",
    "            .drop(\"dropoff_latitude\") \\\n",
    "            .drop(\"passenger_count\") \\\n",
    "            .drop(\"key\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = clean_up(train)\n",
    "\n",
    "train_data = add_time_features(train_data)\n",
    "\n",
    "train_data = add_distance_features(train_data)\n",
    "\n",
    "train_data = drop_col(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:53478118, Columns:21\n"
     ]
    }
   ],
   "source": [
    "columns=len(train_data.dtypes)\n",
    "rows=train_data.count()\n",
    "print (\"Rows:%d, Columns:%d\"%(rows,columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour_of_day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- month_of_year: integer (nullable = true)\n",
      " |-- quarter_of_year: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- night: integer (nullable = true)\n",
      " |-- late_night: integer (nullable = true)\n",
      " |-- abs_diff_longitude: double (nullable = true)\n",
      " |-- abs_diff_latitude: double (nullable = true)\n",
      " |-- manhattan: double (nullable = true)\n",
      " |-- pickup_distance_jfk: float (nullable = true)\n",
      " |-- dropoff_distance_jfk: float (nullable = true)\n",
      " |-- pickup_distance_ewr: float (nullable = true)\n",
      " |-- dropoff_distance_ewr: float (nullable = true)\n",
      " |-- pickup_distance_lgr: float (nullable = true)\n",
      " |-- dropoff_distance_lgr: float (nullable = true)\n",
      " |-- pickup_distance_downtown: float (nullable = true)\n",
      " |-- dropoff_distance_downtown: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.withColumn('index', monotonically_increasing_id())\n",
    "train_data = train_data.sort('index').limit(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train_dataset and test_dataset\n",
    "train_df, test_df = random_split(train_data, [0.9, 0.1])\n",
    "features = [field.name for field in list(train_df.schema) if field.name != \"fare_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss function and optimizer\n",
    "class NYC_Model(nn.Module):\n",
    "    def __init__(self, cols):\n",
    "        super(NYC_Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(cols, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 16)\n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(16)\n",
    "    def forward(self, *x):\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x.squeeze(1)\n",
    "nyc_model = NYC_Model(len(features))\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(nyc_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NYC_Model(\n",
       "  (fc1): Linear(in_features=21, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=16, bias=True)\n",
       "  (fc5): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmoothL1Loss()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.168.1.5\n"
     ]
    }
   ],
   "source": [
    "nodeinfo = ray.nodes()[0]['NodeManagerAddress']\n",
    "\n",
    "# set 'node' resource\n",
    "@ray.remote\n",
    "def set_resource(resource_name, resource_capacity):\n",
    "    #by default, on actor's local node\n",
    "    ray.experimental.set_resource(resource_name,resource_capacity)\n",
    "ray.get(set_resource.remote(nodeinfo, 1))\n",
    "print(nodeinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distributed estimator based on the raydp api\n",
    "#estimator = TorchEstimator(num_workers=4, model=nyc_model, optimizer=optimizer, loss=criterion,\n",
    "#                            feature_columns=features, label_column=\"fare_amount\", \n",
    "#                           batch_size=256, num_epochs=10,placements={nodeinfo:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TorchEstimator(num_workers=1, model=nyc_model, optimizer=optimizer, loss=criterion,\n",
    "                            feature_columns=features, label_column=\"fare_amount\", \n",
    "                           batch_size=256, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# estimator.fit_on_spark(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0: {'num_samples': 906, 'epoch': 1.0, 'batch_count': 4.0, 'train_loss': 10.984497032418156, 'last_train_loss': 28.41286277770996}\n",
      "CPU times: user 17.4 s, sys: 7.43 s, total: 24.8 s\n",
      "Wall time: 19min 16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=84876)\u001b[0m /Users/dr6jl/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py:822: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[2m\u001b[36m(pid=84876)\u001b[0m   return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
      "\u001b[2m\u001b[36m(pid=84876)\u001b[0m /Users/dr6jl/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py:822: UserWarning: Using a target size (torch.Size([138, 1])) that is different to the input size (torch.Size([138])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[2m\u001b[36m(pid=84876)\u001b[0m   return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
      "2020-12-16 22:38:20,241\tERROR worker.py:977 -- Possible unhandled error from worker: \u001b[36mray::ParallelIteratorWorker.par_iter_next_batch()\u001b[39m (pid=84874, ip=192.168.1.5)\n",
      "  File \"python/ray/_raylet.pyx\", line 464, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 419, in ray._raylet.execute_task.function_executor\n",
      "  File \"/Users/dr6jl/Documents/ray-raydp/ray/python/ray/util/iter.py\", line 1158, in par_iter_next_batch\n",
      "    batch.append(self.par_iter_next())\n",
      "  File \"/Users/dr6jl/Documents/ray-raydp/ray/python/ray/util/iter.py\", line 1152, in par_iter_next\n",
      "    return next(self.local_it)\n",
      "StopIteration\n"
     ]
    }
   ],
   "source": [
    "%time estimator.fit_on_spark(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutdown raydp and ray\n",
    "estimator.shutdown()\n",
    "raydp.stop_spark()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
